1) LXC - to share system resources using kernel namespaces:
************************************************************
Docker currently uses LinuX Containers (LXC), which run in the same operating system as its host. 
This allows it to share a lot of the host operating system resources. 
It also manages the networking for you as well.

2) AUFS - Shared commond reads and isolate writes filesystems:
***************************************************************
It also uses AuFS for the file system. 
AuFS is a layered file system, so you can have a read only part, and a write part, and merge those together. 
So you could have the common parts of the operating system as read only, which are shared amongst all of your containers, 
and then give each container its own mount for writing.
sample
******
So let say you have a container image that is 1GB in size. 
If you wanted to use a Full VM, you would need to have 1GB times * number of VMs you want. 
With LXC and AuFS you can share the bulk of the 1GB.
if you have 1000 containers you still might only have a little over 1GB of space for the containers OS, assuming they are all running the same OS image.

3) Less isolation but lighter
******************************
A VM gets its own set of resources allocated to it, and does minimal sharing. 
You get more isolation, but it is much heavier (requires more resources).
With LXC you get less isolation, but they are more lightweight and require less resources. 
So you could easily run 1000 containers on a host, and it doesnt even blink. 
Try doing that with Xen, and unless you have a really big host, I dont think it is possible.

4) Deployment
**************
A full virtualized system usually takes minutes to start, LXC containers take seconds, and sometimes even less than a second.
Deploying a consistent production environment is easier said than done. 
Even if you use tools like chef and puppet, there are always OS updates and other things that change between hosts and environments.
What docker does is it gives you the ability to snapshot the OS into a common image, and makes it easy to deploy on other docker hosts. Locally, dev, qa, prod, etc, all the same image. 
Sure you can do this with other tools, but not as easily or fast.
This is great for unit testing, lets say you have 1000 tests and they need to connect to a database.
in order to not break anything you need to run serially so that the tests dont step on each other (run each test in a transaction and roll back). 
With Docker you could create an image of your database, and then run all the tests in parallel since you know they will all be running against the same snapshot of the database. 
Since they are running in parallel and in LXC containers they could run all on the same box at the same time, and your tests will finish much faster. Try doing that with a full VM.

You start with a base image, and then make your changes, and commit those changes using docker, and it creates an image. 
This image contains only the differences from the base. 
When you want to run your image, you also need the base, and it layers your image on top of the base using a layered file system, in this case AUFS. AUFS merges the different layers together and you get what you want, and you just need to run it. You can keep adding more and more images (layers) and it will keep only saving the diffs.
