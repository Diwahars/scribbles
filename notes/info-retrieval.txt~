Book: INFORMATION RETRIEVAL
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Chapter1:  BOOLEAN RETRIEVAL

1.1 Term Document Matrix:
****************************
To find Shakespeare plays containing Brutus AND Caesar AND NOT Calpurnia
				play1	play2	play3	play4	play5	play6

Antony			1		1		0		0		0		1			
Brutus			1		1		0		1		0		0
Caesar			1		1		0		1		1		1
Calpurnia		0		1		0		0		0		0
Cleopatra		1		0		0		0		0		0
mercy			1		0		1		1		1		1
worser			1		0		1		1		1		0	
		
Brutus AND Caesar AND NOT Calpurnia (! gate)
110100 AND 110111 AND 101111 = 100100 so play1 and play4
This model wont scale for huge data

1.2 Inverted Index
*******************
Sorted Dictionary of terms as KEYS, and lists of document ids where the term occurs as VALUES (POSTINGS)
Deciding between ArrayList and LinkedList is a choice of memory (arraylist doubles up fixed sizes, linked list pointer sizes matter)

Antony,3 	[play1->play2->play6]  
Brutus,3	[play1->play1->play4]
Caesar,5	[play1->play2->play4->play5->play6]
...
Checkout the codebase for InvertedIndex API 
Logic for finding intersection of 2 sorted sets
	STEP1: take a pointer (p1,p2) for each set
	STEP2: compare p1.value = p2.value, if equal, move them to Result Set, else move the pointer with a lower value to the next and repeat STEP1

-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Chapter2:  VOCABULARY AND POSTING LISTS

determination of text encoding like UTF8/16 is important
determination of document unit (like whether a page|chapter|paragraph|email message is a document 0
stopwords like 'is, was, to, at' can be used, but sometimes they make a lot of sense like 'flight to LA' without 'to' is useless 
modern IR systems do not use stopwords owing to good compression algos available.
"We will meet, will meet, meet' - tokens 6 | type(unique words) 3

Token normalization=> is the process of canonicalizing tokens so that matches occur despite superficial differences in the character sequences of the tokens 
The most standard way to normalize is to implicitly create equivalence classes, which are normally named after one member of the set
(USA, U.S.A-> USA),(antidiscriminatory,anti-discriminatory->antidiscriminatory)

Case-sensitiveness=> you can make all tokens lowercase but sometimes phrases like 'General Motors' will not make sense,
Easy technique is to not to lowercase words that appear in middle of sentences, which is to indicate they are done deliberately.

In case of words like 'democracy, democratic, democratization'
Stemming=>Crude technique to remove last part of word to get 'democratic'. Common algo is Porter Algorithm(1980)

Lemmatization=> A bit more intelligent to derive a common word from those combinations.
Processing which does full morphological analysis to accurately identify the lemma for each word

Skip Pointers=> While intersecting posting sets, you can form bands and skip using them. This will avoid 2 many comparisons between the pointers.
A simple tech is to use sqrt(P) as skip length for a posting set of size P.

Biword index=> combination words like 'Stanford University, Pop Music'
Positional Index=>for every term in doc, store its position numbers also
 















