High Performance Browser Networking
________________________________________________________________________________________________________________________________________________________________________
CHAPTER 0: General:
***********************
OSI
****
Physical(wire)->Datalink()->Network(IP)->Transport(TCP)->Session(TLS)->Application(HTTP)

Bandwith
***********
It is dataflow rate=100Mb/s which is dependent more on medium (optic, copper), technology.

RTT:
****
It is roundtrip time which is a product of distance,bandwidth,intermediate stops.

IP
***
is layer below TCP/UDP, has the primary task of delivering datagrams from the source to the destinationhost based on their addresses. 
To do so, the messages are encapsulated within an IP packet which identifies the source and the destination addresses, as well as a number of other routing parameters.
If packets are dropped, its the responsibility of above protocol like TCP to compensate.
____________________________________________________________________________________________________________________________________
NAT:Network Address Translation 
********************************
Given that IP4 is 32 bits, 2^32 is the max number of unique IP addresses globally
IP6 is in progress
NAT is a temp hack which became permanent, its like a telephone operator in your office.
One front-facing globally unique IP/port for LAN (office ntwk) and a table of local IPs./ports (for each employee)

Static NAT:
***********
Mapping an unregistered IP address to a registered IP address on a one-to-one basis. 
Particularly useful when a device needs to be accessible from outside the network.
 
Dynamic NAT:
************
Maps an unregistered IP address to a registered IP address from a group of registered IP addresses.

Overloading:
************
A form of dynamic NAT that maps multiple unregistered IP addresses to a single registered IP address by using different ports. 
This is known also as PAT (Port Address Translation), single address NAT or port-level multiplexed NAT.
 
Overlapping:
************
When the IP addresses used on your internal network are registered IP addresses in use on another network, the router must maintain a lookup table of these addresses so that it can intercept them and replace them with registered unique IP addresses. 
____________________________________________________________________________________________________________________________________________
CHAPTER 2: Networking 101
******************************
1) DELAYS:
***********
    	Probagation delay:    	
        Amount of time required for a message to travel from the sender to receiver, which is a function of distance over speed with which the signal propagates. 
	Propagation time is dictated by the distance and the medium through which the signal travels, the propagation speed is usually within a small constant factor of the speed of light

    	Transmission delay:
        Amount of time required to push all the packet’s bits into the link, which is a function of the packet’s length and data rate of the link. 
	Tansmission delay is dictated by the available data rate of the transmitting link and has nothing to do with the distance between the client and the server. 
        As an example, let’s assume we want to transmit a 10 Mb file over two links: 1 Mbps and 100 Mbps. 
	It will take 10 seconds to put the entire file on the “wire” over the 1 Mbps link and only 0.1 seconds over the 100 Mbps link.

	Processing delay:
	Amount of time required to process the packet header, check for bit-level errors, and determine the packet’s destination. Happens at routers, intermediate hops.
	Queuing delay:
	Amount of time the incoming packet is waiting in the queue until it can be processed. Waiting before routers.
____________________________________________________________________________________________________________________________________________
2) LAST MILE LATENCY:
***********************
	Its the last link between ISP and home/office, which is the slowest.
	tracroute google.com
3) BANDWIDTH:
*****************
	Optical fibers have a distinct advantage when it comes to bandwidth 
	because each fiber can carry many different wavelengths (channels) of light through a process known as wavelength-division multiplexing (WDM). 
	Hence, the total bandwidth of a fiber link is the multiple of per-channel data rate and the number of multiplexed channels.

	The backbones, or the optic fiber links, that form the core data paths of the Internet are capable of moving hundreds of terabits per second. 
	However, the available capacity at the 	edges of the network is much, much less, and varies wildly based on deployed technology: 
	dial-up, DSL, cable, a host of wireless technologies, fiber-to-the-home, and even the performance of the local router. 
____________________________________________________________________________________________________________________________________________
CHAPTER 2: Building blocks of TCP/IP
***************************************
	IP: Internet Protocol is what provides the host-to-host routing and addressing,

	TCP: Transmission Control Protocol, is what provides the abstraction of a reliable network running over an unreliable channel.
	When you work with a TCP stream, you are guaranteed that all bytes sent will be identical with bytes received and that they will arrive in the same order to the client. 
	As such, TCP is optimized for accurate delivery, rather than a timely one.

	HTTP standard never specified TCP as underlying protocol, UDP could've be chosen.

	TCP HANDSHAKE:
	******************
	Every TCP connection starts with 3 step handshake:
	STEP1: SYN: Client picks a random sequence number x and sends a SYN packet, which may also include additional TCP flags and options.
	STEP2: SYN ACK: Server increments x by one, picks own random sequence number y, appends it own set of flags and options, and dispatches the response.
	STEP3: ACK:Client increments both x and y by one and completes the handshake by dispatching the last ACK packet in the handshake

	The delay(58ms London-NY) imposed by the three-way handshake makes new TCP connections expensive to create.
	It is one of the big reasons why connection reuse is a critical optimization for any application running over TCP.

	TCP Fast Open (TFO) is a mechanism, which allows data transfer within the SYN packet, could decrease HTTP transaction network latency
	Congestion Collapse can happen due to resistance between IP (DATAGRAM PROTOCOL) and TCP, leading to servers emiiting multiple copies of same packet.

	FLOW CONTROL:
	*****************
	Flow control is a mechanism to prevent the sender from overwhelming the receiverwith data it may not be able to process.
	The receiver may be busy, under heavy load, or	may only be willing to allocate a fixed amount of buffer space. 
	To address this, each side	of the TCP connection advertises its own receive window (rwnd), which communicates the size of the available buffer space to hold the incoming data.
	TCP connection lifecycle with Slow start to avoid congestion which in turn leads to packet loss.
	TCP primary ambition is to avoid packet loss thus leads to a slow startup.

	3 step handshake (SYN, SYN_ACK, ACK)
	Receive Window
	Congestion Window
	Slow start (doubling congestion window for every round trip RTT)

	Receive Window:[rwnd]
	**********************
	Original TCP allowed only 16bits for window size, but RFC 1323 changed it to 1GB
	Intermediate routers, hubs can strip this option off

	To view/setup this option in linux
	$> sysctl net.ipv4.tcp_window_scaling 
	$> sysctl -w net.ipv4.tcp_window_scaling=1

	Congestion Window:[cwnd]
	**************************
	Sender-side limit on the amount of data the sender can have in flight before receiving an acknowledgment (ACK) from the client.
	To start, the server initializes a new congestion window (cwnd) variable per TCP connection and sets its initial value to a conservative, system-specified value (initcwnd on Linux).

	The maximum amount of data in flight (not ACKed) between the client and the server is the minimum of the rwnd and cwnd variables

	Slow start:
	************
	The maximum amount of data in flight for a new TCP connection is the minimum of the rwnd and cwnd values; hence the server can send up to four network segments to
	the client, at which point it must stop and wait for an acknowledgment. Then, for every received ACK, the slow-start algorithm indicates that the server can increment its cwnd window size by one segment—for every ACKed packet, two new packets can be sent.
	This phase of the TCP connection is commonly known as the “exponential growth” algorithm, as the client and the server are trying to quickly converge on
	the available bandwidth on the network path between them.

	Time taken to reach cwnd size of N = (round trip time)RTT × log2( N/initial cwnd)

	Example:
	• Client and server receive windows: 65,535 bytes (64 KB)
	• Initial congestion window: 4 segments (RFC 2581) (now 10 segments-RFC 6928-Apr 2013)
	• Roundtrip time: 56 ms (London to New York)

	65, 535 bytes/1, 460 bytes ≈ 45 segments
	Time to reach congestion window of 45 segments = 56 ms × log2(45/4 ) = 224 ms

	4RTT & 224ms to reach 64 KB of throughput between the client and server! 
	The fact that the client and server may be capable of transferring at Mbps+ data rates has no effect—that’s slow-start.

	Slow start is OK for large streaming videos but bad for short bursts like page navigation.

	(SSR)
	******
	SSR mechanism, resets the congestion window of a connection after it has been idle for a defined period of time. 
	The rationale is simple: the network conditions may have changed while the connection has been idle, and to avoid congestion, the window is reset to a “safe” default.

	To disable in server:
	• $> sysctl net.ipv4.tcp_slow_start_after_idle
	• $> sysctl -w net.ipv4.tcp_slow_start_after_idle=0


	FULL FLOW FOR A GET REQUEST FOR A  20KB FILE = 19 segments
	***********************************************************
	0 ms 	Client begins the TCP handshake with the SYN packet.
	28 ms 	Server replies with SYN-ACK and specifies its rwnd size.
	56 ms 	Client ACKs the SYN-ACK, specifies its rwnd size, and immediately sends	the HTTP GET request.
	84 ms 	Server receives the HTTP request.
	124 ms 	Server completes generating the 20 KB response and sends 4 TCP segments	before pausing for an ACK (initial cwnd size is 4).
	152 ms 	Client receives 4 segments and ACKs each one.
	180 ms 	Server increments its cwnd for each ACK and sends 8 segments.
	208 ms 	Client receives 8 segments and ACKs each one.
	236 ms 	Server increments its cwnd for each ACK and sends remaining 3 segments.
	264 ms 	Client receives remaining segments, ACKs each one.
 
 	PRR
 	****
	Proportional Rate Reduction (PRR) is a new algorithm specified by RFC 6937, whose goal is to improve the speed of recovery when a packet is lost.
	Developed in Google, now default in linux 3.2+

	Bandwidth-delay product (BDP)
	******************************
	Product of data link’s capacity and its end-to-end delay. 
	The result is the maximum	amount of unacknowledged data that can be in flight at any point in time.
	If either the sender or receiver are frequently forced to stop and wait for ACKs for previous packets, then this would create gaps in the data flow

	Ex1:
	If rwnd=cwnd=16KB RTT=100ms=0.1s
	Max datarate= 16*1024*8/0.1s=1310720 bits/s = 1.31Mb/s

	Ex2:
	If datarate=10Mb/s, RTT=0.1s
	rwnd, cwnd=10*1024*8*0.1=122.1KB

	Head of line blocking:
	***********************
	TCP assigns sequence number to each packet sent from server.
	client sorts them, but if a packet goes missing, the client has to wait to get the full assembly.

	Packet loss:
	*************
	It is necessary as a feedback mechanism in TCP to tune the windows, delivery rate. 
	But scenarios where packet order does not matter do not require TCP
		Audio/Video missing a very small gap
		Computer games missing a previous state packet
		messages where a latest message overrides other
		
	Recommendations to tune TCP
	**********************************
	"Upgrade to latest kernel": 
		simplest bcos they r tuned perfectly.
	“Increasing TCP’s Initial Congestion Window”: 
		A larger starting congestion window allows TCP transfers more data in the first roundtrip and significantly accelerates the window growth—an especially critical optimization for bursty and short-lived connections.
	“Slow-Start Restart”:
		Disabling slow-start after idle will improve performance of long-lived TCP connections, which transfer data in bursts.
	“Window Scaling (RFC 1323)”:
		Enabling window scaling increases the maximum receive window size and allows high-latency connections to achieve better throughput.
	“TCP Fast Open”:
		Allows application data to be sent in the initial SYN packet in certain situations.
		TFO is a new optimization, which requires support both on client and server; investigate if your application can make use of it.

	:$> ss --options --extended --memory --processes --info	

_____________________________________________________________________________________________________________________________________________________________
CHAPTER 3: UDP- User Datagram Protocol:
**********************************************
Came in 1980 after TCP/IP
Chose to omit many features of TCP/IP
Usually called Unreliable Datagram Protocol
Used in DNS
Become first class browser protocol after WebRTC (Web Real Time Communication) (skype)

Datagram
************
A self-contained, independent entity of data carrying sufficient information to be routed from the source to the destination nodes without reliance on earlier exchanges
between the nodes and the transporting network.

NAT & UDP:
*************
The issue with NAT translation, at least as far as UDP is concerned, is precisely the routing table that it must maintain to deliver the data. NAT middleboxes rely on connection
state, whereas UDP has none.
Delivering outbound UDP traffic does not require any extra work, but routing a reply requires that we have an entry in the translation table, which will tell us the IP and port
of the local destination host. Thus, translators have to keep state about each UDP flow, which itself is stateless.

Mechanisms to help:

	STUN: Session Traversal Utilities for NAT [RFC 5389]
	******************************************************
	With this mechanism in place, whenever two peers want to talk to each other over UDP,they will first send binding requests to their respective STUN servers.
	The STUN servers return their respective public IPs over all NATS in their path.
	Now the peers can then use the established public IP and port tuples to exchange data.

	TURN: Traversal Using Relays around NAT protocol [RFC 5766]
	*************************************************************
	Both clients begin their connections by sending an allocate request to the same	TURN server, followed by permissions negotiation.
	Once the negotiation is complete, both peers communicate by sending their data to the TURN server, which then relays it to the other peer.
	Running a relay server is costly, best used as a fallback like 8% of transactions.
_____________________________________________________________________________________________________________________________________________________________

CHAPTER 4: TLS-SSL (Transport layer Security)
*********************************************
Originally from NetScape as SSL2.0/3.0, but IETF formally made it TLS.
TLS runs on top of TCP, 
DTLS The Datagram Transport Layer Security protocol, defined in RFC 6347, runs on top of UDP
SSL is not computationally expensive anymore, even google runs it on commodity hardware, given their user loads.

Encryption
***********
	A mechanism to obfuscate what is sent from one computer to another. Agreed upon public-private key to encrypt content.
Authentication
**************
	A mechanism to verify the validity of provided identification material. 3rd party like VeriSign to declare that this certificate is from 'Bank of America'.
	Chain of Trust.
Integrity
*********
	A mechanism to detect message tampering and forgery. Checksum created from payload from agreed-upon key using TLS in-built MAC(Message Authentication Code) algorithm.

TLS Handshake:
*****************
 Regular TCP SYN,SYN-ACK,ACK
 Server gives public certificate , an optional 32 byte sessionid
 Client makes symmetric key based on Server Public certificate, stores the server sessionid if given
 Server receives Client symmetric key
 Data encrypted based on symmetric key.
 The shared sessionid can make the symmetric key generation a onetime action for subsequent page requests.
 Session Keys can help if the server cannot handle huge loads of unique sessionids, the data now is stored on the client side.	

ALPN:
*******
Since HTTP/HTTPS is prevalent, new extensions like SPDY. WebSockets need to piggyback on them until the intermediary devices like router/hub become compatible.
Application Layer Protocol Negotiation (ALPN) is a TLS extension that introduces support for application protocol negotiation into the TLS handshake
(Figure 4-2), thereby eliminating the need for an extra roundtrip required by the HTTP Upgrade workflow. 
Specifically, the process is as follows:
The client appends a new ProtocolNameList field, containing the list of supported application protocols, into the ClientHello message.
The server inspects the ProtocolNameList field and returns a ProtocolName field indicating the selected protocol as part of the ServerHello message.

NPN: Next Protocol Negotiation
******************************
Next Protocol Negotiation (NPN) is a TLS extension, which was developed as part of the SPDY effort at Google to enable efficient application protocol negotiation during the TLS handshake. 
ALPN is a revised and IETF approved version of the NPN extension. 
In NPN, the server advertised which protocols it supports, and the client then chose and confirmed the protocol. 
In ALPN, this exchange was reversed: the client now specifies which protocols it supports, and the server then selects and confirms the protocol.
The rationale for the change is that this brings ALPN into closer alignment with other protocol negotiation standards.
In other words, ALPN is a successor to NPN, and NPN is deprecated. 

SNI:Server Name Indication
*****************************
'Server Name Indication' is the means by which a single server can host multiple TLS handshakes of web sites with different host names.
Allows the client to indicate the hostname the client is attempting to connect to at the start of the handshake. 
As a result, a web server can inspect the SNI hostname, select the appropriate certificate, and continue the handshake.
Similar to hostname in request headers.

Chain of Trust:
***************
Verifying the chain of trust requires that the browser traverse the chain, starting from the site certificate, and recursively verifying the certificate of the parent until it reaches a trusted root.
Every browser ships with a preinitialized list of trusted certificate authorities (“roots”)

CRL: Certification Revocation List
***********************************
Certificate Revocation List (CRL) is defined by RFC 5280 and specifies a simple mechanism to check the status of every certificate: each certificate authority maintains and
periodically publishes a list of revoked certificate serial numbers.Anyone attempting to verify a certificate is then able to download the revocation list and check the presence
of the serial number within it—if it is present, then it has been revoked.

OCSP: Online Certificate Status Protocol
*****************************************
Made to overcome CRL limitations, client looks up from this DB maintained by CA(Certifying authority) by looking up by serial no, instead of downloading all revoked serial numbers.

Early Termination:
******************
A nearby server can also terminate the TLS session, which means that the TCP and TLS handshake roundtrips are much quicker and the total connection setup latency is greatly
reduced. In turn, the same nearby server can then establish a pool of long-lived, secure connections to the origin servers and proxy all incoming requests and responses to and
from the origin servers. CDNs help in achieving this.

Compression:
************
Compression algorithm is negotiated during the TLS handshake, and compression is applied prior to encryption of each record.
But disable in practice, since its hackable, repeated compression on already compressed data like image/video.
_____________________________________________________________________________________________________________________________________________________________________________________________________

CHAPTER 9: HTTP HISTORY:
************************
In 1991, Berners-Lee outlined the motivation for the new protocol and listed several high-level design goals: file transfer functionality, ability to request an index search of
a hypertext archive, format negotiation, and an ability to refer the client to another server. 

HTTP 0.9
*********
• Client request is a single ASCII character string.
• Client request is terminated by a carriage return (CRLF).
• Server response is an ASCII character stream.
• Server response is a hypertext markup language (HTML).
• Connection is terminated after the document transfer is complete.

Telnet is the simplest HTTP client.

HTTP 1.0
*********
Introduced request/response headers and status codes. 
Both the request and response headers were kept as ASCII encoded, but the response object itself could be of any type: an HTML file, a plain text file, an image, or any other content type.
In addition to media type negotiation, the RFC also documented a number of other commonly implemented capabilities: content encoding, character set support, multipart types, authorization, caching, proxy behaviors, date formats, and more.

HTTP 1.1
*********
The HTTP 1.1 standard resolved a lot of the protocol ambiguities found in earlier versions and introduced a number of critical performance optimizations: 
keepalive connections, chunked encoding transfers, byte-range requests, additional caching mechanisms, transfer encodings, and request pipelining.

HTTP 2.0
********
The primary focus of HTTP 2.0 is on improving transport performance and enabling both lower latency and higher throughput. 
TCP need nort be the primary transport protocol.

_____________________________________________________________________________________________________________________________________________________________________________________________________
CHAPTER10: WEB PERFORMANCE:
***************************
PLT: Page Load time
********************
It is “the time until the loading spinner stops spinning in the browser.” A more technical definition is time to onload event in the browser

On Browser:
**************
HTML->DOM (Document Object Model)
	|   	
	JS		---> RENDER TREE -> LAYOUT -> PAINT
	|
CSS ->CSSOM (CSS Object Model) 
JS, DOM.CSSOM can interwine, deadlock and block, all based on the dependency graph.

• Streaming an HD video from the Yahoo! homepage is bandwidth limited.
• Loading and rendering the Yahoo! homepage is latency limited.

Measuring mechanisms:
*********************
Navigation API - caniuse.com/nav-timing.
Resource waterfall - webpagetest.com

Browsers do 
*************
Document-aware optimization - load critical resources first to get better user experience.
Speculative optimization - learn user patterns and pre-resolve DNS names

Recommendations:
*****************
• Critical resources such as CSS and JavaScript should be discoverable as early as possible in the document.
• CSS should be delivered as early as possible to unblock rendering and JavaScript execution.
• Noncritical JavaScript should be deferred to avoid blocking DOM and CSSOM construction.
• The HTML document is parsed incrementally by the parser; hence the document should be periodically flushed for best performance.
* Hint browser by
	<link rel="dns-prefetch" href="//hostname_to_resolve.com"> => Pre-resolve specified hostname.
	<link rel="subresource" href="/javascript/myapp.js"> => Prefetch critical resource found later on this page. 
	<link rel="prefetch" href="/images/big.jpeg"> => Prefetch resource for this or future navigation.
	<link rel="prerender" href="//example.org/next_page.html"> =>Prerender specified page in anticipation of next user destination.

__________________________________________________________________________________________________________________________________________________________________________________________________
CHAPTER 11: HTTP 1.1
******************
• Persistent connections to allow connection reuse (PRIMARY FEATURE) (single TCP connection to fetch html, required js, css)
• Chunked transfer encoding to allow response streaming
• Request pipelining to allow parallel request processing
• Byte serving to allow range-based resource requests
• Improved and much better-specified caching mechanisms	

Example: Consider a page load with N resources(js,css,html) from server
********
HTTP 1.0 					-> N TCP handshakes + N RTT
HTTP 1.1 KeepAlive 			-> 1 TCP handshake + N RTT
HTTP 1.1 KA+ Pipelining		-> 1 TCP handshake +  1 RTT for all N resources 
Server serial processing of resources 

head-of-line blocking:
**********************
Parallel processing (multiple threads) of N resources into response stream is not possible, 
So if one resource blocks in the pipeline, everyone is blocked.
This results in suboptimal delivery: underutilized network links, server buffering costs.

6 parallel TCP connections from browser:
******************************************
A gap in the HTTP 1.X protocol has forced browser vendors to introduce and maintain a connection pool of up to six TCP streams per host. 
The good news is all of the connection management is handled by the browser itself.
6 is number decided technically, also limited to prevent DOS attacks from single client.
Ironically, this same safety check enables the reverse attack on some browsers: 
if the maximum connection limit is exceeded on the client, then all further client requests are blocked. 
As an experiment, open six parallel downloads to a single host, and then issue a seventh request: it will hang until one of the previous requests has completed

Domain Sharding:
****************
Even 6 TCP connections is not enough, then fetch ur page resources from multiple hosts (sub.domains like static.vijayrc.com)

Concatenation
***************
Multiple JavaScript or CSS files are combined into a single resource 

Spriting
********
Multiple images are combined into a larger, composite image.

Both concatenation and spriting suffer in cache when changes happen to them.

Resource Inlining:
*******************
In practice, a common rule of thumb is to consider inlining for resources under 1–2 KB, as resources below this threshold often incur higher HTTP overhead than the resource itself.
<img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAAAAACH5BAAAAAAALAAAAAABAAEAAAICTAEAOw=="alt="1x1 transparent (GIF) pixel" />
The image is part of the DOM now, so no caching, no separate request.

__________________________________________________________________________________________________________________________________________________________________________________________________
CHAPTER 12: HTTP 2.0
*********************
SPDY made in Google, targeted a 50% reduction in page load time (PLT)
Chrome & some browsers, some sites now support SPDY
SPDY was the catalyst for HTTP 2.0, but SPDY is not HTTP 2.0.

It is expected that HTTP/2.0 will:
	• Substantially and measurably improve end-user perceived latency in most cases, over HTTP 1.1 using TCP.
	• Address the “head of line blocking” problem in HTTP.
	• Not require multiple TCP connections to a server to enable parallelism, thus improving its use of TCP, especially regarding congestion control.
	• Retain the semantics of HTTP 1.1, leveraging existing documentation, including (but not limited to) HTTP methods, status codes, URIs, and where appropriate,header fields.
	• Clearly define how HTTP 2.0 interacts with HTTP 1.x, especially in intermediaries.
	• Clearly identify any new extensibility points and policy for their appropriate

Architecture:
****************
A Binary Framing layer is between TLS and HTTP which communicates in binary encoded messages instead of plaintext
HTTP Semantics do not change so applications can be blissfully unaware.

Stream, Messages and Frames
*********************************
• All communication is performed with a single TCP connection.
• The stream is a virtual channel within a connection, which carries bidirectional messages. Each stream has a unique integer identifier (1, 2, ..., N).
• The message is a logical HTTP message, such as a request, or response, which consists of one or more frames.
• The frame is the smallest unit of communication, which carries a specific type of data—e.g., HTTP headers, payload, and so on.
• Interleave multiple requests in parallel without blocking on any one
• Interleave multiple responses in parallel without blocking on any one
• Use a single single connection to deliver multiple requests and responses in parallel
• Deliver lower page load times by eliminating unnecessary latency
• Remove unnecessary HTTP 1.x workarounds from our application code
• Avoid Head of Line Blocking.
• Each stream can be assigned a 31-bit priority value: (0->[2^31-1]). DOM, CSSOM streams are typically assigned higher priority. 
• A http-2.0 implementing server will send the higher priority streams first
• ALPN - Application Layer Protocol Negotiation is again used to check if HTTP 2.0 can be used.
• HTTP2.0 will take another decade to full establish in Internet.

Flow Control in HTTP2.0:
*************************
• Is hop-by-hop, not end-to-end.
• Is based on window update frames: receiver advertises how many bytes it is prepared to receive on a stream and for the entire connection.
• Flow control window size is updated by a WINDOW_UPDATE frame, which specifies the stream ID and the window size increment value.
• Flow control is directional: receiver may choose to set any window size that it desires for each stream and for the entire connection.
• Flow control can be disabled by a receiver, both for an individual stream or for the entire connection.

Server Push
**************
A powerful new feature of HTTP 2.0 is the ability of the server to send multiple replies for a single client request. That is, in addition to the response for the original request,
the server can push additional resources to the client (Figure 12-4), without the client having to explicitly request each one!
For eg, a server need not wait to push css, js resources with just analysing the html requested by client.
Apache’s mod_spdy looks for X-Associated-Content header,which lists the resources to be pushed.

Header Compression
**********************
• Instead of retransmitting the same data on each request and response, HTTP 2.0 uses “header tables” on both the client and server to track and store previously sent key-value pairs.
• Header tables persist for the entire HTTP 2.0 connection and are incrementally updated both by the client and server.
• Each new header key-value pair is either appended to the existing table or replaces a previous value in the table.
__________________________________________________________________________________________________________________________________________________________________________________________________
CHAPTER 13: Optimising Application Delivery
**************************************************
Best practices like domain sharding, image spriting, multiple TCP connections ..etc applied  for HTTP 1.1 should NOT be applied for HTTP 2.0 
Look into mod_pagespeed of Apachex
__________________________________________________________________________________________________________________________________________________________________________________________________
 CHAPTER 5: Performance of Wireless Network
***************************************************
	Claude Shannon formula: C = BW*log2(1+S/N) 
		where 	C => Channel Capacity
				BW => Bandwidth in Hz
				S => Signal 	
Types of wireless networks
	1) Personal area network (PAN): Within reach of a person peripherals |Cable replacement for Bluetooth, ZigBee, NFC  
	2) Local area network (LAN): Within a building or campus Wireless extension of wired network| IEEE 802.11 (WiFi) 
	3) Metropolitan area network(MAN) : Within a city Wireless inter-network connectivity| IEEE 802.15 (WiMAX)
 	4) Wide area network (WAN): Worldwide Wireless network access| Cellular (UMTS, LTE, etc.)
 
* Doubling in available frequency range will double the data rate—e.g., going from 20 to 40 MHz of bandwidth can double the channel data rate,  which is exactly how 802.11n is improving its performance over earlier WiFi standards!
* Different countries may, and often do, assign different spectrum ranges to the same wireless technology.
* Low-frequency signals travel farther and cover large areas (macrocells), but at the cost of requiring larger antennas and having more clients competing for access. 
   On the other hand, high-frequency signals can transfer more data but won’t travel as far, resulting in smaller coverage areas (microcells) and a requirement for more infrastructure.
* Path loss, or path attenuation, is the reduction in signal power with respect to distance traveled—the exact reduction rate depends on the environment. 
Near-far problem
*******************
A condition in which a receiver captures a strong signal and thereby makes it impossible for the receiver to detect a weaker signal, effectively “crowding out” the weaker signal.
Cell-breathing
***************
A condition in which the coverage area, or the distance of the signal, expands and shrinks based on the cumulative noise and interference levels
Modulation:
**************
Modulation is the process of digital-to-analog conversion, and different “modulation alphabets” can be used to encode the digital signal with different efficiency. 
The combination of the alphabet and the symbol rate is what then determines the final throughput of the channel. 





















