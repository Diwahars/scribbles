OVERVIEW:
********
Apache Spark is a fast and general-purpose cluster computing system. 
Provides high-level APIs in Java, Scala and Python, and an optimized engine that supports general execution graphs. 
Supports a rich set of higher-level tools including 
	Spark SQL for SQL and structured data processing, 
	MLlib for machine learning, 
	GraphX for graph processing, 
	Spark Streaming.


RDD:
***
Hadoop Mapreduce is good but data transfer between iterative steps is always through HDFS persistence which is slow.
Resilient Distributed Dataset is a restricted form of distributed memory.
Immutable, trackable collection of records like git commits (DAG)
Can be built through coarse-grained deterministic operations like map, flatmap, filter
Fault recovery through lineage (tracing back)
High write throughput but needs a lot of memory.


QUICKIE:
********
RDDs can be created from Hadoop InputFormats (such as HDFS files) or by transforming other RDDs. 
Letâ€™s make a new RDD from the text of the README file in the Spark source directory:

./bin/spark-shell
scala> val textFile = sc.textFile("README.md")
scala> textFile.count() // Number of items in this RDD
scala> textFile.first() // First item in this RDD
scala> val linesWithSpark = textFile.filter(line => line.contains("Spark"))
scala> textFile.filter(line => line.contains("Spark")).count() // How many lines contain "Spark"?
scala> textFile.map(line => line.split(" ").size).reduce((a, b) => if (a > b) a else b)
scala> val wordCounts = textFile.flatMap(line => line.split(" ")).map(word => (word, 1)).reduceByKey((a, b) => a + b) //word occurrence count
scala> wordCounts.collect()
scala> wordCounts.cache() //to store in cache 



